{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb152ad-677a-4a9b-8a19-99e7e3d9d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c4e5a2e-f156-4c8c-9a2b-99bc39f6ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc86b85b-9a56-424b-b4d1-048f1ea6ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a353a12-ef08-4811-a8bc-a8637f2b1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ea4ac5-9dc9-47d7-bd56-335e6f3b7651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85659b30-ea5f-4aff-b5b4-4f59565d45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_episodes = 1001\n",
    "output_dir = 'model_output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c714247e-ab49-4139-a015-c83c9879752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62b27257-c9e9-48d8-a0f4-1a7de9095a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent: \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon_decay = 0.995 \n",
    "        \n",
    "        self.epsilon_min = 0.01 \n",
    "        \n",
    "        self.learning_rate = 0.001 \n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self): \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse', optimizer = Adam(lr= self.learning_rate))\n",
    "        return model \n",
    "    \n",
    "    def remeber(self,state, action, reward, next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: \n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "        \n",
    "    def replay(self,batch_size):\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch: \n",
    "            target = reward #gehen mal davon aus wir sind am ende! dann kennen wir den reward \n",
    "            #falls wir noch nicht am ende sind müssen wir den zukünftigen reward schätzen und zum aktuellen dazu rechnen\n",
    "            if not done: \n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0])) \n",
    "            \n",
    "            \n",
    "            target_f = self.model.predict(state) \n",
    "            #print(target_f.shape)\n",
    "            target_f[0][action]= target\n",
    "            #update single target row. \n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0) #model updaten mit einzelnem korrigierten wert\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8e1f0a5-93d0-454f-ba75-1693ba317f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size,action_size)\n",
    "#agent.load(\"model_output/weights550\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b0d4737-60c7-4d06-8434-33de97fa6440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0/1001,score:9,e:1.0\n",
      "episode:1/1001,score:12,e:1.0\n",
      "episode:2/1001,score:37,e:1.0\n",
      "episode:3/1001,score:17,e:0.99\n",
      "episode:4/1001,score:13,e:0.99\n",
      "episode:5/1001,score:36,e:0.99\n",
      "episode:6/1001,score:20,e:0.98\n",
      "episode:7/1001,score:15,e:0.98\n",
      "episode:8/1001,score:16,e:0.97\n",
      "episode:9/1001,score:20,e:0.97\n",
      "episode:10/1001,score:35,e:0.96\n",
      "episode:11/1001,score:28,e:0.96\n",
      "episode:12/1001,score:19,e:0.95\n",
      "episode:13/1001,score:25,e:0.95\n",
      "episode:14/1001,score:11,e:0.94\n",
      "episode:15/1001,score:18,e:0.94\n",
      "episode:16/1001,score:20,e:0.93\n",
      "episode:17/1001,score:14,e:0.93\n",
      "episode:18/1001,score:22,e:0.92\n",
      "episode:19/1001,score:42,e:0.92\n",
      "episode:20/1001,score:10,e:0.91\n",
      "episode:21/1001,score:27,e:0.91\n",
      "episode:22/1001,score:16,e:0.9\n",
      "episode:23/1001,score:47,e:0.9\n",
      "episode:24/1001,score:33,e:0.9\n",
      "episode:25/1001,score:22,e:0.89\n",
      "episode:26/1001,score:12,e:0.89\n",
      "episode:27/1001,score:14,e:0.88\n",
      "episode:28/1001,score:12,e:0.88\n",
      "episode:29/1001,score:11,e:0.87\n",
      "episode:30/1001,score:38,e:0.87\n",
      "episode:31/1001,score:16,e:0.86\n",
      "episode:32/1001,score:29,e:0.86\n",
      "episode:33/1001,score:23,e:0.86\n",
      "episode:34/1001,score:7,e:0.85\n",
      "episode:35/1001,score:21,e:0.85\n",
      "episode:36/1001,score:26,e:0.84\n",
      "episode:37/1001,score:44,e:0.84\n",
      "episode:38/1001,score:12,e:0.83\n",
      "episode:39/1001,score:14,e:0.83\n",
      "episode:40/1001,score:16,e:0.83\n",
      "episode:41/1001,score:8,e:0.82\n",
      "episode:42/1001,score:10,e:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x000001DC7C02DCA0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lukast\\Miniconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 58, in __del__\n",
      "    self.deleter(self.graph)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:43/1001,score:9,e:0.81\n",
      "episode:44/1001,score:45,e:0.81\n",
      "episode:45/1001,score:15,e:0.81\n",
      "episode:46/1001,score:14,e:0.8\n",
      "episode:47/1001,score:47,e:0.8\n",
      "episode:48/1001,score:55,e:0.79\n",
      "episode:49/1001,score:29,e:0.79\n",
      "episode:50/1001,score:15,e:0.79\n",
      "episode:51/1001,score:12,e:0.78\n",
      "episode:52/1001,score:30,e:0.78\n",
      "episode:53/1001,score:15,e:0.77\n",
      "episode:54/1001,score:16,e:0.77\n",
      "episode:55/1001,score:23,e:0.77\n",
      "episode:56/1001,score:17,e:0.76\n",
      "episode:57/1001,score:25,e:0.76\n",
      "episode:58/1001,score:20,e:0.76\n",
      "episode:59/1001,score:44,e:0.75\n",
      "episode:60/1001,score:12,e:0.75\n",
      "episode:61/1001,score:34,e:0.74\n",
      "episode:62/1001,score:41,e:0.74\n",
      "episode:63/1001,score:35,e:0.74\n",
      "episode:64/1001,score:17,e:0.73\n",
      "episode:65/1001,score:16,e:0.73\n",
      "episode:66/1001,score:11,e:0.73\n",
      "episode:67/1001,score:19,e:0.72\n",
      "episode:68/1001,score:21,e:0.72\n",
      "episode:69/1001,score:34,e:0.71\n",
      "episode:70/1001,score:14,e:0.71\n",
      "episode:71/1001,score:12,e:0.71\n",
      "episode:72/1001,score:24,e:0.7\n",
      "episode:73/1001,score:19,e:0.7\n",
      "episode:74/1001,score:32,e:0.7\n",
      "episode:75/1001,score:10,e:0.69\n",
      "episode:76/1001,score:22,e:0.69\n",
      "episode:77/1001,score:14,e:0.69\n",
      "episode:78/1001,score:25,e:0.68\n",
      "episode:79/1001,score:21,e:0.68\n",
      "episode:80/1001,score:24,e:0.68\n",
      "episode:81/1001,score:103,e:0.67\n",
      "episode:82/1001,score:13,e:0.67\n",
      "episode:83/1001,score:14,e:0.67\n",
      "episode:84/1001,score:15,e:0.66\n",
      "episode:85/1001,score:14,e:0.66\n",
      "episode:86/1001,score:12,e:0.66\n",
      "episode:87/1001,score:33,e:0.65\n",
      "episode:88/1001,score:22,e:0.65\n",
      "episode:89/1001,score:22,e:0.65\n",
      "episode:90/1001,score:13,e:0.64\n",
      "episode:91/1001,score:13,e:0.64\n",
      "episode:92/1001,score:14,e:0.64\n",
      "episode:93/1001,score:20,e:0.63\n",
      "episode:94/1001,score:23,e:0.63\n",
      "episode:95/1001,score:30,e:0.63\n",
      "episode:96/1001,score:27,e:0.62\n",
      "episode:97/1001,score:17,e:0.62\n",
      "episode:98/1001,score:74,e:0.62\n",
      "episode:99/1001,score:24,e:0.61\n",
      "episode:100/1001,score:22,e:0.61\n",
      "episode:101/1001,score:29,e:0.61\n",
      "episode:102/1001,score:86,e:0.61\n",
      "episode:103/1001,score:16,e:0.6\n",
      "episode:104/1001,score:48,e:0.6\n",
      "episode:105/1001,score:57,e:0.6\n",
      "episode:106/1001,score:23,e:0.59\n",
      "episode:107/1001,score:18,e:0.59\n",
      "episode:108/1001,score:17,e:0.59\n",
      "episode:109/1001,score:15,e:0.58\n",
      "episode:110/1001,score:12,e:0.58\n",
      "episode:111/1001,score:13,e:0.58\n",
      "episode:112/1001,score:10,e:0.58\n",
      "episode:113/1001,score:31,e:0.57\n",
      "episode:114/1001,score:10,e:0.57\n",
      "episode:115/1001,score:27,e:0.57\n",
      "episode:116/1001,score:26,e:0.56\n",
      "episode:117/1001,score:26,e:0.56\n",
      "episode:118/1001,score:19,e:0.56\n",
      "episode:119/1001,score:15,e:0.56\n",
      "episode:120/1001,score:31,e:0.55\n",
      "episode:121/1001,score:33,e:0.55\n",
      "episode:122/1001,score:29,e:0.55\n",
      "episode:123/1001,score:24,e:0.55\n",
      "episode:124/1001,score:37,e:0.54\n",
      "episode:125/1001,score:21,e:0.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1d8c3ff804cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmaxtimesofar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-e4f2d2d75d04>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[1;31m#print(target_f.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1718\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[0;32m   1719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1720\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1721\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1901\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \"\"\"\n\u001b[1;32m-> 1903\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   5067\u001b[0m               type(self._map_func.output_structure)))\n\u001b[0;32m   5068\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5069\u001b[1;33m     variant_tensor = gen_dataset_ops.flat_map_dataset(\n\u001b[0m\u001b[0;32m   5070\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5071\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\rl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2094\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2097\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FlatMapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "done = False\n",
    "maxtimesofar = 0\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    \n",
    "    for time in range(5000): #10 instead of 5000 \n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done ,_ = env.step(action)\n",
    "        \n",
    "        #für jeden step wo er nicht umfällt bekommt er einen reward von 1 \n",
    "        #falls das spiel endet bekommt er jedoch eine bestrafung von -10\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remeber(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        if done: \n",
    "            print(\"episode:{}/{},score:{},e:{:.2}\".format(e,n_episodes,time,agent.epsilon))\n",
    "            break;\n",
    "        \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "    if e%50 == 0 and time > maxtimesofar:\n",
    "        maxtimesofar = time\n",
    "        agent.save(\"model_output/\" + \"weights\" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41f19b-ebc2-49c4-b22b-6f099f714a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
